import json
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union

import torch
import torchaudio
from torch.utils.data import Dataset
from transformers import Wav2Vec2Processor

try:
    from config import settings
except ImportError:
    # Fallback for standalone testing
    class Settings:
        SAMPLE_RATE = 16000
    settings = Settings()

logger = logging.getLogger(__name__)

class DALIDataset(Dataset):
    """
    PyTorch Dataset for DALI training data.
    Loads audio and text from the JSONL manifest generated by prepare_training_data.py.
    """
    def __init__(
        self, 
        manifest_path: Union[str, Path], 
        processor: Wav2Vec2Processor = None, 
        target_sample_rate: int = 16000,
        min_duration: float = 1.0,
        max_duration: float = 30.0 # Clip long audio for memory efficiency during dev
    ):
        """
        Args:
            manifest_path: Path to the JSONL manifest file
            processor: Hugging Face Wav2Vec2Processor for text encoding
            target_sample_rate: Audio sample rate (default 16kHz for wav2vec2)
            max_duration: Max audio duration in seconds (longer files are truncated or skipped)
        """
        self.manifest_path = Path(manifest_path)
        self.target_sample_rate = target_sample_rate
        self.processor = processor
        self.max_duration = max_duration
        self.min_duration = min_duration
        
        self.entries = self._load_manifest()
        logger.info(f"DALIDataset loaded {len(self.entries)} entries from {manifest_path}")

    def _load_manifest(self) -> List[Dict]:
        entries = []
        if not self.manifest_path.exists():
            logger.error(f"Manifest not found: {self.manifest_path}")
            return []
            
        with open(self.manifest_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    # Basic validation
                    if Path(entry['audio_filepath']).exists():
                        duration = entry.get('duration', 0)
                        if duration >= self.min_duration:
                            entries.append(entry)
                except Exception as e:
                    logger.warning(f"Failed to load manifest line: {e}")
        return entries

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        entry = self.entries[idx]
        
        # Audio params
        audio_path = entry['audio_filepath']
        offset = entry.get('offset', 0.0)
        duration = entry.get('duration', self.max_duration)
        
        # Load Audio (Torchaudio -> Librosa Fallback)
        try:
            # Try torchaudio with frame_offset
            # Need to know SR first to calculate frame offset
            # But torchaudio.info is cheap
            info = torchaudio.info(audio_path)
            orig_sr = info.sample_rate
            
            frame_offset = int(offset * orig_sr)
            num_frames = int(duration * orig_sr)
            
            if num_frames > 0:
                 waveform, sample_rate = torchaudio.load(
                     audio_path, 
                     frame_offset=frame_offset, 
                     num_frames=num_frames, 
                     backend="soundfile"
                 )
            else:
                 # Fallback if duration 0 or missing (legacy)
                 waveform, sample_rate = torchaudio.load(audio_path, backend="soundfile")

            # Resample if necessary
            if sample_rate != self.target_sample_rate:
                resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)
                waveform = resampler(waveform)

            # Convert (C, T) -> (T,)
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0) # Average info mono
            else:
                waveform = waveform.squeeze(0) # Remove channel dim
                
        except Exception as e:
            # Fallback to librosa (handles mp3 better on some systems)
            try:
                import librosa
                import numpy as np
                # librosa loads as mono (T,) by default
                y, sr = librosa.load(
                    audio_path, 
                    sr=self.target_sample_rate, 
                    offset=offset, 
                    duration=duration if duration > 0 else None
                )
                waveform = torch.from_numpy(y)
                # Ensure float32
                if waveform.dtype != torch.float32:
                    waveform = waveform.float()
                    
            except Exception as e2:
                logger.error(f"Error loading audio {audio_path}: {e2}")
                # Return silence
                waveform = torch.zeros(int(duration * self.target_sample_rate))

        # Prepare Text
        text = entry['text']
        
        return {
            "audio": waveform, 
            "text": text,
            "duration": duration,
            "id": entry['id']
        }

class DataCollatorCTCWithPadding:
    """
    Data collator that:
    1. Pads audio inputs to the longest in the batch
    2. Tokenizes text labels and pads them
    """
    def __init__(self, processor: Wav2Vec2Processor, padding=True):
        self.processor = processor
        self.padding = padding

    def __call__(self, features: List[Dict]):
        # split inputs and labels
        audio = [f["audio"].numpy() for f in features]
        texts = [f["text"] for f in features]
        
        # Packing audio
        batch = self.processor(
            audio=audio, 
            sampling_rate=16000, 
            padding=self.padding,
            return_tensors="pt"
        )
        
        # Packing labels
        # Use simple tokenizer call or processor with text arg
        labels_batch = self.processor(
            text=texts, 
            padding=self.padding, 
            return_tensors="pt"
        )
            
        # replace padding with -100 to ignore loss correctly
        if "id" in batch: 
            # cleanup any id field if processor added it? No, batch is from processor(audio)
            pass
            
        labels = labels_batch["input_ids"]
        if "attention_mask" in labels_batch:
            labels = labels.masked_fill(
                labels_batch.attention_mask.ne(1), -100
            )
        
        batch["labels"] = labels
        return batch
