learning_rate: 0.0001
model_name: facebook/wav2vec2-base-960h
processor: !!python/object:transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor
  _in_target_context_manager: false
  audio_tokenizer: null
  chat_template: null
  current_processor: &id001 !!python/object:transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor
    _processor_class: null
    do_normalize: true
    feature_size: 1
    padding_side: right
    padding_value: 0.0
    return_attention_mask: false
    sampling_rate: 16000
  feature_extractor: *id001
  tokenizer: !!python/object:transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer
    SPECIAL_TOKENS_ATTRIBUTES:
    - bos_token
    - eos_token
    - unk_token
    - sep_token
    - pad_token
    - cls_token
    - mask_token
    - additional_special_tokens
    _added_tokens_decoder:
      0: !!python/object:tokenizers.AddedToken
        content: <pad>
        lstrip: true
        normalized: false
        rstrip: true
        single_word: false
        special: false
      1: !!python/object:tokenizers.AddedToken
        content: <s>
        lstrip: true
        normalized: false
        rstrip: true
        single_word: false
        special: false
      2: !!python/object:tokenizers.AddedToken
        content: </s>
        lstrip: true
        normalized: false
        rstrip: true
        single_word: false
        special: false
      3: !!python/object:tokenizers.AddedToken
        content: <unk>
        lstrip: true
        normalized: false
        rstrip: true
        single_word: false
        special: false
    _added_tokens_encoder:
      </s>: 2
      <pad>: 0
      <s>: 1
      <unk>: 3
    _decode_use_source_tokenizer: false
    _in_target_context_manager: false
    _pad_token_type_id: 0
    _processor_class: null
    _special_tokens_map:
      additional_special_tokens: []
      bos_token: <s>
      cls_token: null
      eos_token: </s>
      mask_token: null
      pad_token: <pad>
      sep_token: null
      unk_token: <unk>
    _word_delimiter_token: '|'
    chat_template: null
    clean_up_tokenization_spaces: false
    decoder:
      0: <pad>
      1: <s>
      2: </s>
      3: <unk>
      4: '|'
      5: E
      6: T
      7: A
      8: O
      9: N
      10: I
      11: H
      12: S
      13: R
      14: D
      15: L
      16: U
      17: M
      18: W
      19: C
      20: F
      21: G
      22: Y
      23: P
      24: B
      25: V
      26: K
      27: ''''
      28: X
      29: J
      30: Q
      31: Z
    deprecation_warnings: {}
    do_lower_case: false
    encoder: &id002
      '''': 27
      </s>: 2
      <pad>: 0
      <s>: 1
      <unk>: 3
      A: 7
      B: 24
      C: 19
      D: 14
      E: 5
      F: 20
      G: 21
      H: 11
      I: 10
      J: 29
      K: 26
      L: 15
      M: 17
      N: 9
      O: 8
      P: 23
      Q: 30
      R: 13
      S: 12
      T: 6
      U: 16
      V: 25
      W: 18
      X: 28
      Y: 22
      Z: 31
      '|': 4
    extra_special_tokens: {}
    init_inputs: !!python/tuple []
    init_kwargs:
      bos_token: <s>
      do_lower_case: false
      do_normalize: true
      eos_token: </s>
      name_or_path: facebook/wav2vec2-base-960h
      pad_token: <pad>
      replace_word_delimiter_char: ' '
      return_attention_mask: false
      target_lang: null
      tokenizer_file: null
      unk_token: <unk>
      word_delimiter_token: '|'
    model_input_names:
    - input_ids
    - attention_mask
    model_max_length: 1000000000000000019884624838656
    name_or_path: facebook/wav2vec2-base-960h
    padding_side: right
    replace_word_delimiter_char: ' '
    split_special_tokens: false
    target_lang: null
    tokens_trie: !!python/object:transformers.tokenization_utils.Trie
      _termination_char: ''
      _tokens: !!set
        </s>: null
        <pad>: null
        <s>: null
        <unk>: null
      data:
        <:
          /:
            s:
              '>':
                ? ''
                : 1
          p:
            a:
              d:
                '>':
                  ? ''
                  : 1
          s:
            '>':
              ? ''
              : 1
          u:
            n:
              k:
                '>':
                  ? ''
                  : 1
    total_vocab_size: 32
    truncation_side: right
    verbose: false
    vocab: *id002
vocab_size: null
weight_decay: 0.005
